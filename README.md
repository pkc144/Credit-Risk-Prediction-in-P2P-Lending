
# ğŸ“Š Credit Risk Prediction in P2P Lending

## ğŸ“„ Overview
This project focuses on predicting **credit risk in Peer-to-Peer (P2P) lending** using machine learning techniques.  
It uses the **Bondora loan dataset (2012â€“2016)** to classify whether a borrower is likely to **default** or **repay**.  
The aim is to help investors and lending platforms make better data-driven decisions.

---

## âš™ Features
- Cleaned and preprocessed raw loan data.  
- Applied **class balancing** (oversampling/undersampling).  
- Hybrid feature selection (SHAP + model feature importance).  
- Compared multiple ML models (XGBoost, LightGBM, CatBoost, Random Forest).  
- Generated **AUC curves, confusion matrices, and feature importance tables**.  
- Summarized results in **evaluation metrics tables**.

---

## ğŸ›  Technologies
- **Language**: Python 3.9+  
- **Libraries**:  
  - Data: Pandas, NumPy  
  - ML Models: Scikit-learn, XGBoost, LightGBM, CatBoost  
  - Visualization: Matplotlib, Seaborn, SHAP  
- **Tools**: Jupyter Notebook, VS Code  

---

## ğŸ“ Folder Structure
```

Credit-Risk-Prediction/
â”‚
â”œâ”€â”€ data/                        # Raw / preprocessed datasets
â”‚
â”œâ”€â”€ outputs/                     # Results & evaluation outputs
â”‚   â”œâ”€â”€ Auc\_curves/              # ROC/AUC curves for models
â”‚   â”œâ”€â”€ confusion\_matrices/      # Confusion matrix plots
â”‚   â”œâ”€â”€ evaluation\_metrices\_table/ # Tables with accuracy, F1, AUC etc.
â”‚   â””â”€â”€ top\_15\_features/         # SHAP / importance plots
â”‚
â”œâ”€â”€ src/                         # Source code for pipeline
â”‚   â”œâ”€â”€ evaluate.py              # Model evaluation scripts
â”‚   â”œâ”€â”€ preprocessing.py         # Data preprocessing pipeline
â”‚   â”œâ”€â”€ re                       # (check if placeholder, can remove)
â”‚   â””â”€â”€ utils.py                 # Helper functions
â”‚
â”œâ”€â”€ Train/                       # Training-related scripts
â”‚   â””â”€â”€ (model training files)
â”‚
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ README.md                    # Project documentation
â””â”€â”€ .gitignore                   # Ignored files

````

---

## ğŸš€ Getting Started

### 1. Clone Repository
```bash
git clone https://github.com/your-username/Credit-Risk-Prediction.git
cd Credit-Risk-Prediction
````

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Run Training & Evaluation

* **Preprocessing**

  ```bash
  python src/preprocessing.py
  ```
* **Model Training (inside Train/)**

  ```bash
  python Train/train_model.py
  ```
* **Evaluation**

  ```bash
  python src/evaluate.py
  ```

Outputs (confusion matrices, AUC curves, metrics tables, SHAP features) will be saved in the **`outputs/`** folder.

---

## ğŸ“Š Model Evaluation

### Models Compared

* Logistic Regression
* Decision Tree
* Random Forest
* XGBoost
* LightGBM
* CatBoost

### Example Metrics

* **Best Model**: Random Forest with hybrid feature selection
* **AUC**: 0.91
* **F1-score**: 0.82

---

## ğŸ“ˆ Results & Visualizations

* Confusion Matrices â†’ `outputs/confusion_matrices/`
* ROC / AUC Curves â†’ `outputs/Auc_curves/`
* Feature Importance (Top 15) â†’ `outputs/top_15_features/`
* Evaluation Summary â†’ `outputs/evaluation_metrices_table/`

---

## ğŸ”® Future Work

* Try **deep learning models** (TabNet, Autoencoders).
* Add **cost-sensitive learning** to handle imbalanced data.
* Deploy as a **FastAPI/Flask service** for real-time scoring.




ğŸ‘‰ Do you also want me to **regenerate `requirements.txt`** (scikit-learn, xgboost, catboost, shap, etc.) so anyone can set it up in one command?
```
